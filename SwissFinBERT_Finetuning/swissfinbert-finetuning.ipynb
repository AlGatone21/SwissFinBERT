{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c5d72f4",
   "metadata": {},
   "source": [
    "# Finetuning SwissFinBERT\n",
    "\n",
    "this code has been adapted form a LightingAI template\n",
    "https://lightning.ai/docs/pytorch/1.4.0/notebooks/lightning_examples/text-transformers.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0873756d-262a-4525-b809-4e0b3a1e63dd",
   "metadata": {},
   "source": [
    "![](figures/finetuning-ii.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09213821-b2b4-402e-adf8-7c7fe4ec57cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1 Loading the dataset into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e2228-5f0b-4fb9-b762-df26c2052b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from local_dataset_utilities import download_dataset, load_dataset_into_to_dataframe, partition_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44defef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"finetuning_gpt_labelled_vfinal_balanced.xlsx\") # load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b9473",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"Sentence\", \"Label\"] # rename the columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26250db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_matchings = { \"BUY\" : 2, \"HOLD\" : 1, \"SELL\" : 0}\n",
    "\n",
    "df[\"Label\"] = df[\"Label\"].apply(lambda x: label_matchings[x]) # convert the labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"Sentence\", \"Label\"]] # keep only the columns we need\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b004c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()# check the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7c5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "df_train, df_test_val = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the test set into test and validation sets\n",
    "df_test, df_val = train_test_split(df_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting dataframes\n",
    "\n",
    "df_train.to_csv(\"train_df.csv\", index=False)\n",
    "df_test.to_csv(\"test_df.csv\", index=False)\n",
    "df_val.to_csv(\"val_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f30a1-b433-4304-a18d-8d03abd42b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train_df.csv\")\n",
    "df_val = pd.read_csv(\"val_df.csv\")\n",
    "df_test = pd.read_csv(\"test_df.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "876736c1-ae27-491c-850b-050507fa02b5",
   "metadata": {},
   "source": [
    "# 2 Tokenization and Numericalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afe0cca0-bac4-49ed-982c-14c998e578d1",
   "metadata": {},
   "source": [
    "**Load the dataset via `load_dataset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa66c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"train_df.csv\",\n",
    "        \"validation\": \"val_df.csv\",\n",
    "        \"test\": \"test_df.csv\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b201159-f3fa-4649-8076-eff8bc5535d3",
   "metadata": {},
   "source": [
    "**Tokenize the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea762ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"scherrmann/GermanFinBert_SC_Sentiment\") # load the tokenizer for the model\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(batch): # function to tokenize the text\n",
    "    return tokenizer(batch[\"Sentence\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb392cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = dataset.map(tokenize_text, batched=True, batch_size=None) # tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4103c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset # delete the original dataset to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef894c-978f-47f2-9d61-cb6a9f38e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea67091-aeb7-46c1-871f-638ce58d8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ff16488-abe6-48af-9b03-868b457b0ea3",
   "metadata": {},
   "source": [
    "# 3 Set Up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807b068-7d8f-4055-a26a-177e07dea4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define the dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, dataset_dict, partition_key=\"train\"):\n",
    "        self.partition = dataset_dict[partition_key]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.partition[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.partition.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb08f3-ef77-4351-8b19-42d99dd24f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(tokenized, partition_key=\"train\")\n",
    "val_dataset = Dataset(tokenized, partition_key=\"validation\")\n",
    "test_dataset = Dataset(tokenized, partition_key=\"test\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=36, # set the batch size, increased to prevent overfitting\n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=36, # set the batch size, increased to prevent overfitting\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=36, # set the batch size, increased to prevent overfitting\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78e774ab-45a0-4c48-ad61-a3d0e1927ef4",
   "metadata": {},
   "source": [
    "# 4 Initializing GFinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28ddbe-1a96-4c24-9f5c-40ffdca4a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "# Load the model to be fine-tuned\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"scherrmann/GermanFinBert_SC_Sentiment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "def1cf25-0a7d-4bb2-9419-b7a8fe1c1eab",
   "metadata": {},
   "source": [
    "## 5 Finetuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "534f7a59-2c86-4895-ad7c-2cdd675b003a",
   "metadata": {},
   "source": [
    "**Wrap in LightningModule for Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Define the LightningModule, used for fine-tuning the model\n",
    "class LightningModel(L.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-5, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        self.val_acc = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "        self.test_acc = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "        self.val_f1 = torchmetrics.classification.MulticlassF1Score(3, top_k=1, average='micro')\n",
    "        self.test_f1 = torchmetrics.classification.MulticlassF1Score(3, top_k=1, average='micro')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n",
    "                       labels=batch[\"Label\"])        \n",
    "        self.log(\"train_loss\", outputs[\"loss\"])\n",
    "        return outputs[\"loss\"]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n",
    "                       labels=batch[\"Label\"])        \n",
    "        self.log(\"val_loss\", outputs[\"loss\"], prog_bar=True)\n",
    "        \n",
    "        logits = outputs[\"logits\"]\n",
    "        predicted_labels = torch.argmax(logits, 1)\n",
    "        self.val_acc(predicted_labels, batch[\"Label\"])\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "        self.val_f1(predicted_labels, batch[\"Label\"])\n",
    "        self.log(\"val_f1\", self.val_f1, prog_bar=True)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n",
    "                       labels=batch[\"Label\"])        \n",
    "        \n",
    "        logits = outputs[\"logits\"]\n",
    "        predicted_labels = torch.argmax(logits, 1)\n",
    "        self.test_acc(predicted_labels, batch[\"Label\"])\n",
    "        self.log(\"accuracy\", self.test_acc, prog_bar=True)\n",
    "        self.test_f1(predicted_labels, batch[\"Label\"])\n",
    "        self.log(\"f1\", self.test_f1, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=1e-5)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize the model\n",
    "lightning_model = LightningModel(model) # initialize the model in the LightningModule class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dab813-e1fc-47cd-87a1-5eb8070699c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# Define the callbacks and logger, used for logging and saving the model during training\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        save_top_k=1, mode=\"max\", monitor=\"val_acc\"\n",
    "    )  # save top 1 model based on the best accuracy\n",
    "]\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492aa043-02da-459e-a266-091b34254ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=5, # set the number of epochs\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\", # use the GPU\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=3,\n",
    ")\n",
    "\n",
    "trainer.fit(model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45bb3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load the logged metrics\n",
    "metrics = pd.read_csv(\"/logs/my-model/version_40/metrics.csv\")\n",
    "\n",
    "# Plot the loss curves\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot train and validation loss against steps\n",
    "ax1.plot(metrics[\"step\"], metrics[\"train_loss\"], label=\"Train Loss\", color='black')\n",
    "ax1.set_xlabel(\"Step\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Loss Curve\")\n",
    "ax1.legend(loc=\"upper right\")\n",
    "\n",
    "# Create a secondary x-axis to show epochs\n",
    "ax2 = ax1.twiny()\n",
    "ax2.set_xlim(ax1.get_xlim())\n",
    "ax2.set_xticks(metrics[\"step\"][metrics[\"epoch\"].drop_duplicates().index])\n",
    "ax2.set_xticklabels(metrics[\"epoch\"].drop_duplicates())\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795778a-70d2-4b04-96fb-598eccbcd1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(lightning_model, dataloaders=train_loader, ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca0af1-106e-4ef7-9793-478d580af827",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(lightning_model, dataloaders=val_loader, ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb92de4-d483-4627-b9f3-f0bba0cddd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d7260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model.model.save_pretrained(\"SwissFinBERT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4141d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login # log in to the Hugging Face Hub\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ebcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model.model.push_to_hub(\"AlGatone21/SwissFinBERT\", use_temp_dir=True) # push the model to the Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\"AlGatone21/SwissFinBERT\", use_temp_dir=True) # push the tokenizer to the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a06b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model.model.config.label2id = {\"SELL\": 0, \"HOLD\": 1, \"BUY\": 2} # set the label mappings\n",
    "lightning_model.model.config.id2label = {0: \"SELL\", 1: \"HOLD\", 2: \"BUY\"} # set the label mappings\n",
    "lightning_model.model.config.push_to_hub(\"SwissFinBERT\", use_temp_dir=True) # push the model configuration to the Hugging Face Hub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
